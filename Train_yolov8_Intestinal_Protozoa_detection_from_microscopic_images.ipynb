{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satishsalyal/Data-Cleaning-Practical-Examples/blob/master/Train_yolov8_Intestinal_Protozoa_detection_from_microscopic_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOLOV8 Intestinal Protozoa Detection**\n",
        "This notebook explains training custom YOLOv8 model for  Intestinal Protozoa Detection. I am using \"Chula-ParasiteEgg-11 Dataset\" from IEEDataPort . https://ieee-dataport.org/competitions/ parasitic-egg-detection-and-classification-microscopic-images\n",
        "\n",
        "This is a initial version of custom trianing with YOLOv8. Currently YOLOv8 is the newest state-of-the-art YOLO model that can be used for object detection, image classification, and instance segmentation tasks.\n",
        "\n",
        "I am using Google Colab for trianing. If you are intrested to check custom trianing with YOLOv8, please follow this link 😀 https://github.com/satishsalyal/neuralnetwork"
      ],
      "metadata": {
        "id": "sw8ayn9yFlNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Train YOLOv8 to Detect Intestinal parasites from microscopic images\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics)\n",
        "\n",
        "Ultralytics YOLOv8 is the latest version of the YOLO (You Only Look Once) object detection  model developed by Ultralytics. \n",
        " -  designed to be fast, accurate, and easy to use\n",
        " -  an excellent choice for a wide range of object detection tasks.    \n",
        " -  trained on large datasets\n",
        " -  capable of running on a variety of hardware platforms, from CPUs to GPUs.\n",
        "\n",
        "\n",
        "## Pro Tip: Before Running the Script Please Make Sure you Select the Run Time as GPU\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to \n",
        " - `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. \n",
        " -This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- Setting Up Google Colab\n",
        "- YOLOV8 Installation\n",
        "- Mounting Google Drive\n",
        "- Create Intestinal_Protozoa_Detetcion.yaml (dataset config file)\n",
        "- Training Our Custom Intestinal Protozoa Detetcion Model\n",
        "- Validate Custom Model\n",
        "- Metrics\n",
        "- Inference with Custom Model\n",
        "\n",
        "**Let's begin!**"
      ],
      "metadata": {
        "id": "RX9qYq67QWpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setting Up Google Colab\n",
        "Google Colab is an online environment similar to Jupiter notebook where you can train deep learning models on GPU/TPU. The free plan of Google Colab allows you to train the deep learning model for up to 12 hrs before the runtime disconnects. By visiting the runtime section change run type to GPU."
      ],
      "metadata": {
        "id": "0IB2Tj-RF4vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to check and monitoring of NVIDIA GPU devices. \n",
        "!nvidia-smi "
      ],
      "metadata": {
        "id": "cv4b6yBHGABT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ea8c02-e8ac-44dc-efc2-5e02f808c5cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 11 05:16:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. YOLOV8 Installation:\n",
        "We used to clone the repo or use torchhub to work with Yolov5. The recent YoloV8 has been released as pip package, so we don't need to clone any repo. This package alone installs ever dependency for yolov8.\n",
        "\n",
        "The Pip install of the ultralytics package including all requirements.txt in a Python>=3.7.0 environment, including PyTorch>=1.7."
      ],
      "metadata": {
        "id": "yD_Cz2S0GCk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing package to work with yolov8\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "Zba7Y-RPGF_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c56e024-5433-4e40-fd7c-8b63189e0f7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.73-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.4.4)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from ultralytics) (5.9.4)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.10.1)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (2.0.0+cu118)\n",
            "Collecting sentry-sdk\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.15.1+cu118)\n",
            "Collecting thop>=0.1.1\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (4.65.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (4.7.0.72)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.39.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (3.10.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (3.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (3.25.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->ultralytics) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: sentry-sdk, thop, ultralytics\n",
            "Successfully installed sentry-sdk-1.19.1 thop-0.1.1.post2209072238 ultralytics-8.0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Mounting Google Drive\n",
        "I have uploaded **Intestinal_Protoza folder** to Google drive in the 'MyDrive/datasets/intestial_Protozoa/' path, I will mount drive using the below code. (It will ask you to enter the authorization code that you can by clicking the link that will appear below). The annotated data has been divided in such a way that the images and the labels (text files) are separate."
      ],
      "metadata": {
        "id": "a9pDqcJUGIhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "f17I8GLXGMHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883d20f7-a4c2-4595-c67d-1b9df3f9faa4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the uploaded data in drive\n",
        "!ls '/content/drive/MyDrive/datasetIPS'"
      ],
      "metadata": {
        "id": "_ZMFWeH4GOa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c60b063-4249-4712-c7de-56b6826fd0a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coco128.yaml  test  train  valid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##      Unzipped the Dataset"
      ],
      "metadata": {
        "id": "58DS5NhIjOls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -u  /content/drive/MyDrive/dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T084nxsjUae",
        "outputId": "b6cd464b-75f0-4da5-d7d0-ff42686a1242"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/dataset.zip, /content/drive/MyDrive/dataset.zip.zip or /content/drive/MyDrive/dataset.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOLOV8 format:\n",
        "The format for Yolov8 is same as Yolov5. The YOLO format, with one .txt file per image (if no objects in image, no .txt file is required). The *.txt file specifications are:\n",
        "\n",
        "One row per object Each row is class x_center y_center width height format.\n",
        "\n",
        "Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height. Class numbers are zero-indexed (start from 0)."
      ],
      "metadata": {
        "id": "VyQtRFY5GQzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the size of images and displaying them\n",
        "import numpy as np\n",
        "import cv2\n",
        "# Image shape in Training\n",
        "image = cv2.imread('/content/drive/MyDrive/datasetIPS/train/images/Trichuris-trichiura_0999_jpg.rf.e6a40bbd5dda3f9e84021180cc9e188d.jpg')\n",
        "height = np.size(image, 0)\n",
        "width = np.size(image, 1)\n",
        "print (\"shape of the training image {}, {}\".format(height, width))\n",
        "# Image shape in validation\n",
        "'''image = cv2.imread('/content/drive/MyDrive/datasetIPS/Intestinal_Protozoat/valid/images/maksssksksss67.png')\n",
        "height = np.size(image, 0)\n",
        "width = np.size(image, 1)\n",
        "print (\"shape of the validation image {}, {}\".format(height, width))'''"
      ],
      "metadata": {
        "id": "_ASLkmVJGUQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dispying with different width\n",
        "from IPython.display import Image \n",
        "Image(filename='/content/drive/MyDrive/datasets/Intestinal_Protozoa/train/images/5e353e347af50726986e84c0.jpeg', width=300) "
      ],
      "metadata": {
        "id": "PhHro_V1KNVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create Intestinal_Protozoa_Detetcion.yaml (dataset config file)\n",
        "\"Inteasinal Protozoa Detectin\" is having the images are split as follows: Train: 990 = 70% Valid: 294 = 20% Test: 136 =10% (I am not using test datatset, instaed i will test with soem video from the internet) Total = 1420 images\n",
        "\n",
        "mask_dataset/face_mask_detetcion.yaml, created below, is the dataset config file that defines:\n",
        "\n",
        "the dataset root directory path and relative paths to train / val / test image directories (or *.txt files with image paths)\n",
        "\n",
        "nc: the number of classes\n",
        "\n",
        "names: a list of class names"
      ],
      "metadata": {
        "id": "oxdjP4PVKP6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I will write the contents of the cell to a file\n",
        "#%%writefile /content/drive/MyDrive/datasetIPS/coco128.yaml\n",
        "!ls '/content/drive/MyDrive/datasetIPS/coco128.yaml'\n",
        "\n"
      ],
      "metadata": {
        "id": "oBW088g0KUUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334cf0dd-6474-495f-b046-a2939d95df75"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasetIPS/coco128.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training Our Custom Intestinal Protozoa-detection-from-microscopic-images:\n",
        "I am using a YOLOv8m pretrained model for custom training with my dataset.\n",
        "\n",
        "Parameters:\n",
        "\n",
        " - specify the path to the data configuration file\n",
        " - specify a path to weights to start transfer learning from. yolov8m.pt (starting from Pretrained weights)\n",
        " - input image size\n",
        " - Size of a batch (model weights are updated with each batch).\n",
        " - No of epochs.\n",
        "\n",
        "It will cache images for faster training, cache images in \"ram\" (default) or \"disk"
      ],
      "metadata": {
        "id": "wLBhvwI_KWwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "# model = YOLO(\"yolov8m.yaml\")  # build a new model from scratch\n",
        "model = YOLO(\"yolov8m.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "results = model.train(data=\"/content/drive/MyDrive/datasetIPS/coco128.yaml\", epochs=10, imgsz=640)  # train the model"
      ],
      "metadata": {
        "id": "Zs-yT0M9Kt3V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "d54d36d7-4134-41ad-d4aa-fed8f2778b88"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-47eb8bf123d8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load a model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model = YOLO(\"yolov8m.yaml\")  # build a new model from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov8m.pt\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load a pretrained model (recommended for training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Validation:\n",
        "Validate trained YOLOv8n model accuracy on the validatio dataset. No argument need to passed as the model retains it's training data and arguments as model attributes."
      ],
      "metadata": {
        "id": "zlaw_odLLR8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.val()  # evaluate model performance on the validation set"
      ],
      "metadata": {
        "id": "x8pPoKVRLY9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 Metrics:\n",
        "The trainig mAP per class and over all is good and testing results on video are also good. If we train for epochs, adding more data and playing with hyperparameters can improve performance of the model."
      ],
      "metadata": {
        "id": "CrKzJCgoLdNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dislaying metrics for train data\n",
        "from IPython.display import Image\n",
        "from IPython.display import display\n",
        "x = Image(filename='runs/detect/train2/F1_curve.png') \n",
        "y = Image(filename='runs/detect/train2/PR_curve.png') \n",
        "z = Image(filename='runs/detect/train2/confusion_matrix.png') \n",
        "display(x, y,z)"
      ],
      "metadata": {
        "id": "MtB8fAalLhy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8 Inference:"
      ],
      "metadata": {
        "id": "_Zm3HhwiMVB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the latest trained files\n",
        "!ls 'runs/detect/train2/weights'"
      ],
      "metadata": {
        "id": "y79qdWcyMMim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# loading the trianed model\n",
        "model = YOLO(\"runs/detect/train2/weights/best.pt\")  # load a custom model"
      ],
      "metadata": {
        "id": "yWCJG_KsMZgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the model on a video\n",
        "# results = model(\"/content/drive/MyDrive/datasets/mask_dataset/mask_testing.mp4\") \n",
        "!yolo task=detect mode=predict model=\"runs/detect/train2/weights/best.pt\" source=\"/content/drive/MyDrive/datasets/Intestinal_Protozoa/mask_testing.mp4\""
      ],
      "metadata": {
        "id": "y_0Mbb59McAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion:\n",
        "Based on the inference results, the trained model is doing a great job. We can still imrpove it by using large yolov8 models, additional data and hyperparameter changes."
      ],
      "metadata": {
        "id": "vugzvd1tMhuW"
      }
    }
  ]
}